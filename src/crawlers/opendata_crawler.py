from config.common_imports import *
from config.logging_config import setup_logging
from config.settings import BASE_URL, DATA_TYPES, REQUEST_PARAMS

logger = setup_logging("crawler.log")

async def parse(url):
    """ì…ë ¥ëœ URLì„ HTMLë¡œ ë¹„ë™ê¸°ì ìœ¼ë¡œ íŒŒì‹±"""
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, timeout=20) as response:
                html = await response.text()
                return bs(html, "html.parser")
        except aiohttp.ClientError as err:
            logger.error(f"Request error occurred: {err} - URL: {url}")
            return None

def return_search_url(dType, currentPage=1):
    """ê²€ìƒ‰ URLì„ ìƒì„±"""
    params = REQUEST_PARAMS.copy()
    params.update({
        'dType': dType,
        'currentPage': currentPage
    })
    search_url = BASE_URL+'&'.join([f"{key}={value}" for key, value in params.items()])
    return search_url

def update_url_page(url, new_page):
    """URLì˜ í˜ì´ì§€ ë²ˆí˜¸ë§Œ ì—…ë°ì´íŠ¸"""
    return re.sub(r'currentPage=\d+', f'currentPage={new_page}', url)

def get_page_count(soup):
    """ì´ í˜ì´ì§€ ìˆ˜ë¥¼ ì¶”ì¶œ"""
    page_div = soup.select("nav.pagination")
    
    if not page_div:
        # Paginationì´ ì—†ëŠ” ê²½ìš° ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ë‹¤ê³  ê°€ì •
        logger.warning("No search results found.")
        return 0  # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° 0ì„ ë°˜í™˜
    
    # í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ì´ ìˆëŠ”ë° a íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš°
    page_links = page_div[0].select("a")
    
    if not page_links:
        # <strong> íƒœê·¸ê°€ í•˜ë‚˜ë§Œ ì¡´ì¬í•˜ê³  ê·¸ ì•ˆì— '1'ì´ë¼ëŠ” ìˆ«ìê°€ ìˆëŠ” ê²½ìš°
        active_page = page_div[0].select("strong.active")
        if active_page and active_page[0].text.strip() == '1':
            return 1  # í˜ì´ì§€ê°€ 1ê°œë¿ì´ë¯€ë¡œ 1ì„ ë°˜í™˜
        
    try:
        last_a = page_div[0].select("a.control.last")
        
        if not last_a:
            # "ë§ˆì§€ë§‰ í˜ì´ì§€" ë²„íŠ¼ì´ ì—†ëŠ” ê²½ìš°
            count_a = len(page_div[0].select("a"))
            return count_a + 1  # í•´ë‹¹ í˜ì´ì§€ì—ì„œ ë°œê²¬ëœ a íƒœê·¸ì˜ ìˆ˜ë¥¼ í˜ì´ì§€ ìˆ˜ë¡œ ì‚¬ìš©
            
        # a íƒœê·¸ì˜ onclick ì†ì„±ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì¶”ì¶œ
        onclick_value = last_a[0].get('onclick')
        numbers = re.findall(r'\d+', onclick_value)
        page_count = int(numbers[0])
        
    except Exception as e:
        logger.error(f"An error occurred while trying to get the page count: {e}")
        page_count = 1  # ê¸°ë³¸ì ìœ¼ë¡œ 1í˜ì´ì§€ë¡œ ê°„ì£¼
    
    return page_count

async def get_page_data(dType, soup):
    result_list = soup.select("div.result-list")
    temp_list = []
    for result in result_list:
        li_list = result.find_all("li")
        for li in li_list:
            dt = li.find("dl").find("dt")
            title = dt.find("span", class_="title").text.strip()
            info_url = dt.find("a")["href"]
            tagset = ','.join(span.text.strip() for span in dt.find_all("span", class_="tagset"))
            div_info_list = li.find("div", class_="info-data").find_all("p")
            info_dict = {}
            for info in div_info_list:
                tit = info.find("span", class_="tit").text.strip()
                if (tit == "ì œê³µê¸°ê´€"):
                    if (dType == "LINKED"):
                        data = info.find("span", class_="data").text.strip()
                    else:
                        data = info.find("span", class_="data").find("span", class_="esHighlight").text.strip()
                elif(tit == "í‚¤ì›Œë“œ"):
                    data = list(info.children)[-1].strip()
                else:
                    data = info.find("span", class_="data").text.strip()
                info_dict[tit] = data
                
            temp_data = {
                "ë°ì´í„°ëª…": title,
                "ìƒì„¸ë§í¬": info_url,
                "ì œê³µê¸°ê´€": info_dict.get("ì œê³µê¸°ê´€", ""),
                "ìˆ˜ì •ì¼": info_dict.get("ìˆ˜ì •ì¼", ""),
                "ì¡°íšŒìˆ˜": info_dict.get("ì¡°íšŒìˆ˜", ""),
                "í‚¤ì›Œë“œ": info_dict.get("í‚¤ì›Œë“œ", "")
            }
            
            if dType == "API":
                temp_data["ë°ì´í„°í¬ë§·"] = tagset
                temp_data["í™œìš©ì‹ ì²­"] = info_dict.get("í™œìš©ì‹ ì²­", "")
            else:
                temp_data["í™•ì¥ì"] = tagset
            if dType == "FILE":
                temp_data["ì£¼ê¸°ì„± ë°ì´í„°"] = info_dict.get("ì£¼ê¸°ì„± ë°ì´í„°", "")
                temp_data["ë‹¤ìš´ë¡œë“œ"] = info_dict.get("ë‹¤ìš´ë¡œë“œ", "")
            
            # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            nonl = lambda x:" ".join(x.split())
            def tel_no_format(telno: str) -> str:
                if telno and not re.findall(r"\d{2,3}-\d{4}-\d{4}", telno):
                    return "-".join(re.search(r"^(02.{0}|01.{1}|[0-9]{3})([0-9]+)([0-9]{4})", telno).groups())
                return telno

            detail_soup = await parse(BASE_URL + info_url)
            board = detail_soup.select_one("#contents").select_one("div.data-search-view")
            temp_data["ì„¤ëª…"] = board.select_one(".cont").text.strip()
            
            table_data = {}
            for data_table_row in board.select("div.file-meta-table-pc > table > tr"):
                cols = data_table_row.find_all(["th", "td"])
                for th, td in zip(cols[::2], cols[1::2]):
                    if th.name != "th" or td.name != "td":
                        logger.error(f"data table error. {info_url}\nth: {th}\ntd: {td}")
                        break
                    key = nonl(th.text)
                    value = nonl(td.text)
                    if key == "ê´€ë¦¬ë¶€ì„œ ì „í™”ë²ˆí˜¸" and not value:
                        telno = re.search(r"telNo.+\"([\d.]+)\"", td.select_one("script").text).groups()[0]
                        value = tel_no_format(telno)
                    table_data[key] = value
            
            common_columns = {"ì„¤ëª…", "ë“±ë¡ì¼"}
            additional_columns = set()
            if dType == "FILE":
                additional_columns = {"ì—…ë°ì´íŠ¸ ì£¼ê¸°", "ì œê³µí˜•íƒœ", "URL"}
            elif dType == "LINKED":
                temp_data["ë°”ë¡œê°€ê¸° ë§í¬"] = board.select_one("div.btn-util > a")['href']
                additional_columns = {"ë°”ë¡œê°€ê¸° íšŸìˆ˜", "ë°”ë¡œê°€ê¸° ë§í¬"}
            
            selected_columns = common_columns.union(additional_columns) & table_data.keys()
            temp_data.update({key: table_data[key] for key in selected_columns})
            
            temp_list.append(temp_data)
    
    return temp_list
    
async def get_list(dType, df):
    logger.info(f"{dType} ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤")
    base_url = return_search_url(dType)
    soup = await parse(base_url)
    page_count = get_page_count(soup)
    
    async def fetch_page_data(page):
        max_retries = 10
        for attempt in range(max_retries):
            try:
                if page == 1:
                    url = base_url
                else:
                    url = update_url_page(base_url, page)
                soup = await parse(url)
                return await get_page_data(dType, soup)
            except Exception as exc:
                if attempt < max_retries - 1:
                    logger.warning(f'Page {page} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ì¬ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})')
                    await asyncio.sleep(5)  # ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸°
                else:
                    logger.error(f'Page {page} ìˆ˜ì§‘ ì‹¤íŒ¨: {exc}')
                    return None

    tasks = [fetch_page_data(i) for i in range(1, page_count + 1)]
    for future in tqdm_asyncio.as_completed(tasks, total=page_count, desc=f"Collecting {dType} data"):
        try:
            result = await future
            if result is not None:
                page_df = pd.DataFrame(result)
                df = pd.concat([df, page_df], ignore_index=True)
        except Exception as exc:
            logger.error(f'ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {exc}')
            
    org = REQUEST_PARAMS['org']
    output_dir = os.path.join('', 'data')
    os.makedirs(output_dir, exist_ok=True)
    today = datetime.now().strftime('%y%m%d')
    filename = f"{org}_{dType}_ê³µê³µë°ì´í„°í¬í„¸_í¬ë¡¤ë§_{today}.xlsx"
    file_path = os.path.join(output_dir, filename)
    save_to_excel(df, file_path)
    return df

def save_to_excel(df, filepath):
    df.to_excel(filepath, index=True)
    logger.info(f"Data saved to {filepath}")

async def main():
    # Load sub_organizations list
    sub_orgs_path = os.path.join("data", "sub_organizations.json")
    
    # Check if file exists
    if not os.path.exists(sub_orgs_path):
        logger.error(f"'{sub_orgs_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    with open(sub_orgs_path, "r", encoding="utf-8") as file:
        sub_orgs = json.load(file)

    # If no organizations are found
    if not sub_orgs:
        logger.warning("ê¸°ê´€ ëª©ë¡ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    # Loop through each organization
    for org in sub_orgs:
        logger.info(f"ğŸ“Œ í˜„ì¬ ê¸°ê´€: {org}")

        # Update config settings dynamically
        REQUEST_PARAMS["org"] = org

        # Initialize dataframes
        dataframes = {}

        # Loop through each data type (FILE, API, LINKED)
        for data_type, columns in DATA_TYPES.items():
            df = pd.DataFrame(columns=columns)
            dataframes[data_type] = await get_list(data_type, df)

        # Log completion
        logger.info(f"âœ… ê¸°ê´€ '{org}' ë°ì´í„° í¬ë¡¤ë§ ì™„ë£Œ.")

    return dataframes

if __name__ == "__main__":
    asyncio.run(main())